{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91744d1-e99d-4fef-8adc-a7918ee3a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea85061-31e9-4c34-afb9-7a4bb55a4a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0), CudaDevice(id=1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0040fde7-3565-4241-95c2-0b4b34d8e2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0), CudaDevice(id=1)]\n",
      "[[1. 2. 3. 4.]\n",
      " [5. 6. 7. 8.]]\n",
      "{CudaDevice(id=0), CudaDevice(id=1)}\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Check available devices\n",
    "print(jax.devices())\n",
    "\n",
    "# Create two example arrays\n",
    "x = jnp.arange(8).reshape(2, 4)\n",
    "y = jnp.ones((2, 4))\n",
    "\n",
    "# Define a simple function to run on each device\n",
    "def add_fn(x, y):\n",
    "    return x + y\n",
    "\n",
    "# pmap distributes computation over devices\n",
    "p_add = jax.pmap(add_fn, axis_name='i')\n",
    "\n",
    "# Run distributed addition\n",
    "z = p_add(x, y)\n",
    "\n",
    "print(z)\n",
    "print(z.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ecf508-e253-4508-9414-19b12f106135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [CudaDevice(id=0), CudaDevice(id=1)]\n",
      "\n",
      "Gradient results:\n",
      "[[[ 1.1103526   0.11962527 -1.1234112   0.01695487]\n",
      "  [ 0.45118243 -0.14754063 -0.5328977   0.25843456]\n",
      "  [ 0.2522853  -0.5790242   2.7051687  -0.99971235]\n",
      "  [ 0.06262801 -0.19462773  1.4745142   0.7654087 ]]\n",
      "\n",
      " [[ 0.8668243   0.16151243  0.43444693 -1.0751637 ]\n",
      "  [-1.852232    2.1205912   0.22519185  0.86963576]\n",
      "  [-0.3140492   0.3806017   0.16959918 -0.01568799]\n",
      "  [-0.16057253 -0.5774332  -0.46479216  0.675294  ]]]\n",
      "\n",
      "Devices: {CudaDevice(id=0), CudaDevice(id=1)}\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Make sure you have 2 devices\n",
    "print(\"Devices:\", jax.devices())\n",
    "\n",
    "# Simple matrix multiply function\n",
    "def matmul_fn(W, X):\n",
    "    return jnp.dot(W, X)\n",
    "\n",
    "# Loss function (mean squared output)\n",
    "def loss_fn(W, X):\n",
    "    Y = matmul_fn(W, X)\n",
    "    return jnp.mean(Y ** 2)\n",
    "\n",
    "# pmap-ed gradient function (distributed)\n",
    "p_grad_fn = jax.pmap(jax.grad(loss_fn), axis_name='i')\n",
    "\n",
    "# Initialize data â€” one batch per GPU\n",
    "key = jax.random.PRNGKey(0)\n",
    "W = jax.random.normal(key, (2, 4, 4))   # shape (num_devices, ...)\n",
    "X = jax.random.normal(key, (2, 4, 4))\n",
    "\n",
    "# Compute distributed gradients\n",
    "grads = p_grad_fn(W, X)\n",
    "\n",
    "print(\"\\nGradient results:\")\n",
    "print(grads)\n",
    "print(\"\\nDevices:\", grads.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcc2b96-5aef-40d7-aa13-03e20d31868c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "import jax.numpy as jnp\n",
    "\n",
    "devices = jax.devices()\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((2,)), ('dp',))\n",
    "\n",
    "x = jnp.ones((10, 10))\n",
    "sharding = NamedSharding(mesh, PartitionSpec('dp', None))\n",
    "x_sharded = jax.device_put(x, sharding)\n",
    "\n",
    "from jax import jit\n",
    "\n",
    "@jit\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "f(x_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5d39f0-88e3-4085-948a-39a25417501f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanjha/.conda/envs/bosonic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jaxquantum as jqt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14378930-7310-4ede-88f8-b8e3f2e5a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 11:50:01.121452: E external/xla/xla/service/rendezvous.cc:92] [id=0] This thread has been waiting for `initialize clique for rank 1; clique=devices=[0,1]; stream=0; groups=[[0,1]]; root_device=-1; num_local_participants=2; incarnations=[]; run_id=1126119018` for 10 seconds and may be stuck. All 2 threads joined the rendezvous, however the leader has not marked the rendezvous as completed. Leader can be deadlocked inside the rendezvous callback.\n"
     ]
    }
   ],
   "source": [
    "devices = jax.devices()\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((2,)), ('dp',))\n",
    "\n",
    "x = jqt.identity(10).data\n",
    "sharding = NamedSharding(mesh, PartitionSpec('dp', None))\n",
    "x_sharded = jax.device_put(x, sharding)\n",
    "\n",
    "import jax.scipy as jsp\n",
    "\n",
    "jsp.linalg.eigh(x_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c690c-f725-492a-8668-d04f050c8e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [CudaDevice(id=0), CudaDevice(id=1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1016 11:54:40.489626 1700608 spmd_partitioner.cc:630] [spmd] Involuntary full rematerialization. The compiler was not able to go from sharding {devices=[1,2]<=[2]} to {maximal device=0} without doing a full rematerialization of the tensor for HLO operation: %get-tuple-element = f32[1,4]{1,0} get-tuple-element(%param), index=3, sharding={devices=[1,2]<=[2]}. You probably want to enrich the sharding annotations to prevent this from happening.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "import diffrax\n",
    "\n",
    "# --- Setup 2-GPU mesh ---\n",
    "devices = jax.devices()\n",
    "print(\"Available devices:\", devices)\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((2,)), ('d',))\n",
    "\n",
    "# --- Define a small linear system dy/dt = A y ---\n",
    "n = 8\n",
    "key = jax.random.PRNGKey(0)\n",
    "A = jax.random.normal(key, (n, n), dtype=jnp.float32)\n",
    "y0 = jax.random.normal(key, (n,), dtype=jnp.float32)\n",
    "\n",
    "# --- Shard A and y across GPUs ---\n",
    "A_sharding = NamedSharding(mesh, PartitionSpec('d', None))\n",
    "y_sharding = NamedSharding(mesh, PartitionSpec('d'))\n",
    "A = jax.device_put(A, A_sharding)\n",
    "y0 = jax.device_put(y0, y_sharding)\n",
    "\n",
    "# --- Define RHS and solver ---\n",
    "def rhs(t, y, A):\n",
    "    return A @ y  # local matmul per shard\n",
    "\n",
    "\n",
    "def run_solver(A, y0):\n",
    "    term = diffrax.ODETerm(rhs)\n",
    "    solver = diffrax.Tsit5()\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        t0=0.0,\n",
    "        t1=1.0,\n",
    "        dt0=0.1,\n",
    "        y0=y0,\n",
    "        args=A,\n",
    "        saveat=diffrax.SaveAt(t1=True),\n",
    "    )\n",
    "    return sol.ys\n",
    "\n",
    "# --- Run across 2 GPUs ---\n",
    "with mesh:\n",
    "    y_final = run_solver(A, y0)\n",
    "\n",
    "print(\"Final y(t=1):\", y_final)\n",
    "print(\"Sharded across devices:\", y_final.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4ab39-1d7b-4dc8-8121-a6ea5581f68c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
